import os, subprocess, shlex, tempfile, re, textwrap
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2:3b")

app = FastAPI(title="PentestGPT Wrapper")

class GenReq(BaseModel):
    prompt: str

YAML_FENCE = re.compile(r"(?s)```(?:yaml|yml)?\s*(.*?)```")

def _extract_yaml(text: str) -> str | None:
    if not text:
        return None
    m = YAML_FENCE.search(text)
    if m:
        return m.group(1).strip()
    # fallback ingênuo: pegue do primeiro 'playbook:' ou 'steps:'
    for marker in ("playbook:", "steps:"):
        i = text.find(marker)
        if i != -1:
            return text[i:].strip()
    return None

def _yaml_prompt(prompt: str) -> str:
    return textwrap.dedent(f"""
    You are PentestGPT. Output ONLY a minimal YAML playbook. No explanations, no code fences.
    Required keys:
      playbook: <short-id>
      steps:
        - id: <id>
          type: <http_get|run_script|...>
          # add minimal args if needed
    Task:
    {prompt}
    """).strip()

def _ollama_generate_yaml(prompt: str) -> str:
    """Gera YAML direto via Ollama (endpoint OpenAI-compatível)."""
    url = f"{OLLAMA_BASE_URL}/v1/chat/completions"
    data = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": "You produce only YAML playbooks. No explanations."},
            {"role": "user", "content": _yaml_prompt(prompt)}
        ],
        "stream": False
    }
    try:
        r = requests.post(url, json=data, timeout=60)
        r.raise_for_status()
        j = r.json()
        content = j["choices"][0]["message"]["content"]
        yaml_text = _extract_yaml(content) or (content or "").strip()
        if not yaml_text:
            raise RuntimeError("empty yaml from ollama")
        return yaml_text
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"ollama fallback failed: {e}")

@app.post("/generate_playbook")
def generate_playbook(req: GenReq):
    if not req.prompt.strip():
        raise HTTPException(400, "prompt required")

    # 1) Tenta CLI do PentestGPT (não falhe em exit!=0)
    p = (
        "You are PentestGPT. Produce ONLY a YAML playbook. No explanations.\n\n"
        f"{req.prompt}\n"
    )
    with tempfile.NamedTemporaryFile("w+", delete=False) as tf:
        tf.write(p); tf.flush()
        prompt_file = tf.name

    cmd = f"pentestgpt --ollama {shlex.quote(OLLAMA_MODEL)} < {shlex.quote(prompt_file)}"
    try:
        proc = subprocess.run(
            cmd, shell=True, text=True,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, timeout=60
        )
        text = (proc.stdout or "").strip()
        yaml_text = _extract_yaml(text) or (text if text.startswith("playbook:") else None)
        if yaml_text:
            return {"playbook": yaml_text, "metadata": {"model": OLLAMA_MODEL, "source": "cli"}}
        # se não conseguiu extrair YAML do CLI, faça fallback
    except subprocess.TimeoutExpired:
        # timeout no CLI — cai para fallback
        pass
    except Exception:
        # qualquer outra falha — cai para fallback
        pass

    # 2) Fallback robusto: gerar YAML via Ollama diretamente
    yaml_text = _ollama_generate_yaml(req.prompt)
    return {"playbook": yaml_text, "metadata": {"model": OLLAMA_MODEL, "source": "ollama-fallback"}}

class FeedbackReq(BaseModel):
    playbook: str
    result: dict

@app.post("/feedback")
def feedback(req: FeedbackReq):
    return {"ok": True, "received": {"len_playbook": len(req.playbook), "has_result": bool(req.result)}}
